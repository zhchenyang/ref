# Preface

Statistical learning refers to a set of tools for making sense of complex datasets. In recent years, we have seen a straggering increase in the scale and scope of data collection across virtually all areas of science and industry. As a result, statistical learning has become a critical toolkit for anyone who wishse to understand data -- and as more and more of today's jobs involve data, this means that statistical learning is fast becoming a critical toolkit for everyone.

# Introduction

## An Overview of Statistical Learning

**Statistical learning** refers to vast set of tools for **understanding data**. 

These tool can be classified as **supervised** and **unsupervised**. Broadly speaking:

1. supervised statistical learning involves building a statistical model for predicting, or estimationg, an **output** base on one or more **inputs**.

2. unsupervised learning, there are inputs but no supervising output; nevertheless we can learn relationships and structure from such data.

## A Brief History of Statistical Learning

Though the term **Statistical Learning** is fairly new, many of the concepts that underlie the field were developed long ago. At the begining of the nineteenth century, the method **least squares** are developed, implementing the earlist form of what is now now known as **linear regression**. The approach was first successfully applied to problems in astronomy. Linear regression is used for predict quantitative values, such ad an individual's salary. 

In order to predict qualitative values, such as whether a patient survives or dies, **linear discriminant analysis** was proposed in 1936. In the 1940s, various authors put forth an alternative approach, **logistic regression**. In the early 1970s, the term **generalized linear model** was developed to describe an entire class of statistical learning methods that include both linear and logistic regression as special cases.

By the end of the 1970s, many more techniques for learning from data were availabe. However, they were almost exclusively **linear** methods because fitting **non-linear** relationships was computationally difficult at the time. By the 1980s, computing technology had finally impoved sufficiently that non-linear methods were no longer computationally prohibitive. In the mid 1980s, **clssification** and **regression tree** were developed, followed shortly by **generalized additive** models. **Neural networks** gained popualarity in 1980s, and **support vector machines** arose in the 1990s.

Since that time, statistical learning has emerged「缓缓出现」as a new subfield in statistics, focused on supervised and unsupervised modeling and prediction. 
